path = "C:\\Users\\pearl\\Downloads\\Analytics edge\\Text Analytics"
setwd(path)
wiki = read.csv("wiki.csv", stringsAsFactors = FALSE)
library(tm)
library(SnowballC)
colnames(wiki)
wiki$Vandal= as.factor(wiki$Vandal)
table(wiki$Vandal)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded,removeWords,stopwords("english"))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
length(stopwords("english"))
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))

corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved,removeWords,stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
wordsRemoved
nrow(wordsRemoved)
ncol(wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
spl = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
table(test$Vandal)
618/(618+545)
library(rpart)
library(rpart.plot)
wcart = rpart(Vandal ~ ., data = train , method = "class")
prp(wcart)
pred = predict(wcart, newdata = test)
pred.prob = pred[,2]
table(test$Vandal, pred.prob>=0.5)
630/(630+533)
# Our model performs really bad on the prediction so we are going to try something else
#we are looking for webaddresses in the pages as we hypothesize that web adresses mean that pages were vandalized
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http", wiki$Added, fixed = TRUE),1,0)
table(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, spl == TRUE)
wikiTest2 = subset(wikiWords2, spl == FALSE)

wcart2 = rpart(Vandal~., data = wikiTrain2, method = "class")
prp(wcart2)
pred2 = predict(wcart2, newdata = wikiTest2)
pred2.prob = pred2[,2]
table(wikiTest2$Vandal,pred2.prob >=0.5)
(609+57)/(609+9+488+57)
#number of words added and removed is predictive, perhaps more so than the actual words themselves. 
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)

wikiTrain3 = subset(wikiWords2, spl == TRUE)
wikiTest3 = subset(wikiWords2, spl == FALSE)
wcart3 = rpart(Vandal~., data = wikiTrain3, method = "class")
prp(wcart3)
pred3 = predict(wcart3, newdata = wikiTest3)
pred3.prob = pred3[,2]
table(wikiTest3$Vandal, pred3.prob>0.5)
(514+248)/(514+248+297+104)
#Q3
wikiWords3 = wikiWords2

#Adding two original variables to new data matrix
wikiWords3$Minor = wiki$Minor

wikiWords3$Loggedin = wiki$Loggedin

#Splitting data set into test and train

wikiTrain3= subset(wikiWords3, spl == TRUE)
wikiTest3 = subset(wikiWords3, spl == FALSE)

wcart4 = rpart(Vandal~., data = wikiTrain3, method = "class")
prp(wcart4)
pred4 = predict(wcart4, newdata = wikiTest3)
pred4.prob = pred4[,2]
table(wikiTest3$Vandal, pred4.prob > 0.5)
(595+241)/(595+23+304+241)









